{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## School project - 5MLRE\n",
    "The following notebook was created for a school project to create an anime recommendation system. The subject and the questions are available in the appendix.\n",
    "\n",
    "The group members who participated in this project are:\n",
    "- AMIMI Lamine\n",
    "- BEZIN Th√©o\n",
    "- LECOMTE Alexis\n",
    "- PAWLOWSKI Maxence\n",
    "\n",
    "### Main index\n",
    "1. Data analysis\n",
    "2. **Collaborative filtering (you are here)**\n",
    "3. Content-based filtering\n",
    "4. _Appendix_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Collaborative filtering\n",
    "In the previous notebook, we loaded, cleaned and studied the [MyAnimeList](https://myanimelist.net/) datasets. Now that we know them better, we will start to create the recommendation system using collaborative filtering. Collaborative filtering is a technique that filters out items that a user might like based on feedback from similar users. There are two sub-techniques: User-based collaborative filtering and article-based collaborative filtering.\n",
    "\n",
    "### Index\n",
    "<ol type=\"A\">\n",
    "  <li>Notebook initialization</li>\n",
    "  <li>Collaborative filtering: unfiltered training</li>\n",
    "  <li>Collaborative filtering: filtered training</li>\n",
    "  <li>Getting the Top-N</li>\n",
    "  <li>Conclusion of the collaborative filtering</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A - Notebook initialization\n",
    "### A.1 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS and filesystem\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# Data\n",
    "import pandas\n",
    "from matplotlib import pyplot\n",
    "import matplotx\n",
    "\n",
    "# Model processing\n",
    "import surprise\n",
    "\n",
    "# Console output\n",
    "from colorama import Style\n",
    "\n",
    "# Misc.\n",
    "from ast import literal_eval\n",
    "\n",
    "# Local files\n",
    "sys.path.append(os.path.join(os.pardir, os.pardir))\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2 - Package initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.rcParams.update(pyplot.rcParamsDefault)\n",
    "pyplot.style.use(matplotx.styles.dracula)  # Set the matplotlib style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3 - Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filesystem paths\n",
    "PARENT_FOLDER = Path.cwd()\n",
    "DATA_FOLDER = (PARENT_FOLDER / \"..\" / \"..\" / \"data\").resolve()\n",
    "MODELS_FOLDER = (PARENT_FOLDER / \"..\" / \"..\" / \"models\").resolve()\n",
    "TEMP_FOLDER = (PARENT_FOLDER / \"..\" / \"..\" / \"temp\").resolve()\n",
    "\n",
    "# Plots\n",
    "FIG_SIZE = (12, 7)\n",
    "\n",
    "# Misc.\n",
    "RANDOM_STATE = 2077"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.4 - Datasets loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_reader = surprise.Reader(line_format=\"user item rating\", sep=\",\", rating_scale=(-1, 10), skip_lines=1)\n",
    "# data = surprise.Dataset.load_from_file(file_path=(DATA_FOLDER / \"rating2.csv\"), reader=data_reader)\n",
    "\n",
    "# Load a smaller sample of the dataset instead of the 8M rows\n",
    "data = pandas.read_csv((DATA_FOLDER / \"rating.csv\"), dtype={\"user_id\": str, \"anime_id\": str})\n",
    "# data = data[data[\"rating\"] >= 0]\n",
    "data = data.head(n=125_000)  # We use `head` instead of `samples`.\n",
    "\n",
    "data_reader = surprise.Reader(rating_scale=(-1, 10))\n",
    "data = surprise.Dataset.load_from_df(df=data[[\"user_id\", \"anime_id\", \"rating\"]], reader=data_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_anime = pandas.read_csv(DATA_FOLDER / \"anime_cleaned.csv\", converters={\"genre_split\": literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = pandas.Series(data=data_anime[\"rank_num_ratings\"].values, index=data_anime[\"anime_id\"]).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing sets. This can take a while...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Building train/test sets...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Building LeaveOneOut sets...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Building full sets...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Preparing the similarities model...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "evaluator = helpers.ml.ModelEvaluator(dataset=data, rankings=rankings, models_folder=MODELS_FOLDER, seed=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B - Collaborative filtering: unfiltered training\n",
    "As we explained earlier, Collaborative filtering is a method used to make personalized recommendations by analyzing a user's past preferences or behaviors and comparing them to those of similar users. There are two sub-techniques: User-based collaborative filtering and article-based collaborative filtering.\n",
    "\n",
    "- User-based: focuses on finding similar users that are looking like the target user in terms of preferences, liked items and user's navigation.\n",
    "- Item-based: focuses on finding similar items based on the user's previous interactions with other items.\n",
    "\n",
    "User-based is more relevant for entertainment-related items, as this approach would recommend items that other users with similar preferences have liked. There are a lot of parameters in terms of preference nuances. Item-based recommendations are more pertinent to online shops, which recommend products based on their characteristics. We are talking about individual tastes here.\n",
    "\n",
    "In our case, user-based filtering should give better results. But in this notebook we will test our models with both methods.\n",
    "\n",
    "### B.1 - Slope One\n",
    "Slope One is a collaborative filtering algorithm designed for recommendations. Its lightweight and simple design calculates the average difference between the user's items rating and uses this information to predict the user's potential rating on an unseen article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mTesting \"Slope One\".\u001B[39m\n",
      "Computing metrics...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2mCalculating the accuracy (RMSE, MAE)...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2mBuilding the top-N...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Fitting on the LOOCV...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Fitting on the full set...\u001B[37m\u001B[2m\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "\u001B[0m\n",
      "\u001B[1mBest params:\u001B[22m \u001B[2m\u001B[37m{}\u001B[0m\n",
      "\u001B[1mRMSE:\u001B[22m 2.223912\n",
      "\u001B[1mMAE:\u001B[22m 1.432369\n",
      "\n",
      "\u001B[1mHit rate:\u001B[22m 0.404858%\n",
      "\u001B[1mHit rate per rating value:\u001B[22m\n",
      "Rating\tHit rate\n",
      "10.0\t2.136752%\n",
      "\u001B[1mCumulative hit rate (min_rating=3.0):\u001B[22m 0.488281%\n",
      "\u001B[1mAverage reciprocal hit rank:\u001B[22m 0.0020917678812415654\n",
      "\u001B[1mUser coverage (num_users=1235, min_rating=3.0):\u001B[22m 100.000000%\n",
      "\u001B[1mDiversity:\u001B[22m 0.666667\n",
      "\u001B[1mNovelty:\u001B[22m 3281.476531\n",
      "\n",
      "\u001B[0mTesting of the \"Slope One\" model successfully completed in 0:10:14.751439.\n",
      "Grid search: N/A\n",
      "Training and testing: 0:00:04.693023\n",
      "Top-N building: 0:10:06.570479\n"
     ]
    }
   ],
   "source": [
    "evaluator.run_model(name=\"Slope One\", model=surprise.SlopeOne, hyper_params=None, measure_key=\"rmse\", override=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2 - KNN Basic\n",
    "KNN Basic (K-Nearest Neighbors) is another collaborative filtering algorithm used for recommandation systems. It consists of finding the most similar \"K\" items or users based on a similarity metric. It then calculates the weighted average of the ratings of the items found to predict the user's rating for the target item or recommend items based on the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mTesting \"KNN Basic\".\u001B[39m\n",
      "Running GridSearchCV...\u001B[37m\u001B[2m\n",
      "Computing metrics...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2mCalculating the accuracy (RMSE, MAE)...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\u001B[37m\u001B[2mBuilding the top-N...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Fitting on the LOOCV...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\u001B[37m\u001B[2m   > Fitting on the full set...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\u001B[33mWarning: 2296265/6597142 (34.806966%) predictions were impossible! \u001B[39m\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "\u001B[33mWarning: 2397381/6802312 (35.243620%) predictions were impossible! \u001B[39m\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "\u001B[0m\n",
      "\u001B[1mBest params:\u001B[22m \u001B[2m\u001B[37m{'k': 40, 'min_k': 1, 'sim_options': {'name': 'pearson_baseline', 'user_based': False}}\u001B[0m\n",
      "\u001B[1mRMSE:\u001B[22m 2.229013\n",
      "\u001B[1mMAE:\u001B[22m 1.382239\n",
      "\n",
      "\u001B[1mHit rate:\u001B[22m 0.323887%\n",
      "\u001B[1mHit rate per rating value:\u001B[22m\n",
      "Rating\tHit rate\n",
      "-1.0\t0.480769%\n",
      "8.0\t0.787402%\n",
      "10.0\t0.427350%\n",
      "\u001B[1mCumulative hit rate (min_rating=3.0):\u001B[22m 0.292969%\n",
      "\u001B[1mAverage reciprocal hit rank:\u001B[22m 0.0020242914979757085\n",
      "\u001B[1mUser coverage (num_users=1235, min_rating=3.0):\u001B[22m 100.000000%\n",
      "\u001B[1mDiversity:\u001B[22m 0.911111\n",
      "\u001B[1mNovelty:\u001B[22m 3091.652830\n",
      "\n",
      "\u001B[0mTesting of the \"KNN Basic\" model successfully completed in 1:23:05.853355.\n",
      "Grid search: 1:06:50.734911\n",
      "Training and testing: 0:00:07.844338\n",
      "Top-N building: 0:16:03.530733\n"
     ]
    }
   ],
   "source": [
    "evaluator.run_model(\n",
    "    name=\"KNN Basic\",\n",
    "    model=surprise.KNNBasic,\n",
    "    hyper_params={\n",
    "        \"k\": [20, 40, 60],\n",
    "        \"min_k\": [1, 2, 3, 5],\n",
    "        \"sim_options\": {\n",
    "            \"name\": [\"cosine\", \"msd\", \"pearson\", \"pearson_baseline\"],\n",
    "            \"user_based\": [True, False]\n",
    "        }\n",
    "    },\n",
    "    measure_key=\"rmse\",\n",
    "    override=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.3 - KNN With Means\n",
    "KNN With Means is a variant of the KNN Basic algorithm. This time, the algorithm adjusts the previously calculated weighted average by adding the overall average user or article rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mTesting \"KNN With Means\".\u001B[39m\n",
      "Running GridSearchCV...\u001B[37m\u001B[2m\n",
      "Computing metrics...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2mCalculating the accuracy (RMSE, MAE)...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\u001B[37m\u001B[2mBuilding the top-N...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Fitting on the LOOCV...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\u001B[37m\u001B[2m   > Fitting on the full set...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "\u001B[0m\n",
      "\u001B[1mBest params:\u001B[22m \u001B[2m\u001B[37m{'k': 40, 'min_k': 5, 'sim_options': {'name': 'pearson_baseline', 'user_based': True}}\u001B[0m\n",
      "\u001B[1mRMSE:\u001B[22m 2.160327\n",
      "\u001B[1mMAE:\u001B[22m 1.351401\n",
      "\n",
      "\u001B[1mHit rate:\u001B[22m 2.348178%\n",
      "\u001B[1mHit rate per rating value:\u001B[22m\n",
      "Rating\tHit rate\n",
      "7.0\t1.570681%\n",
      "8.0\t2.362205%\n",
      "9.0\t0.862069%\n",
      "10.0\t7.692308%\n",
      "\u001B[1mCumulative hit rate (min_rating=3.0):\u001B[22m 2.832031%\n",
      "\u001B[1mAverage reciprocal hit rank:\u001B[22m 0.008375104427736006\n",
      "\u001B[1mUser coverage (num_users=1235, min_rating=3.0):\u001B[22m 95.303644%\n",
      "\u001B[1mDiversity:\u001B[22m 1.000000\n",
      "\u001B[1mNovelty:\u001B[22m 1006.727677\n",
      "\n",
      "\u001B[0mTesting of the \"KNN With Means\" model successfully completed in 1:21:18.351094.\n",
      "Grid search: 1:13:45.173980\n",
      "Training and testing: 0:00:04.716624\n",
      "Top-N building: 0:07:24.538414\n"
     ]
    }
   ],
   "source": [
    "evaluator.run_model(\n",
    "    name=\"KNN With Means\",\n",
    "    model=surprise.KNNWithMeans,\n",
    "    hyper_params={\n",
    "        \"k\": [20, 40, 60],\n",
    "        \"min_k\": [1, 2, 3, 5],\n",
    "        \"sim_options\": {\n",
    "            \"name\": [\"cosine\", \"msd\", \"pearson\", \"pearson_baseline\"],\n",
    "            \"user_based\": [True, False]\n",
    "        }\n",
    "    },\n",
    "    measure_key=\"rmse\",\n",
    "    override=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.4 - KNN With Z-Score\n",
    "KNN With Z-Score is another variant of the KNN algorithm that takes into account the average ratings and standard deviations of users or items for predictions. In addition to the previous steps, the algorithm calculates the Z-Score by subtracting the average score and dividing the result by the standard deviation on the weighted average. With this method, this algorithm normalizes the ratings by trends and variabilities, which means better accuracy for predictions and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mTesting \"KNN With Z-Score\".\u001B[39m\n",
      "Running GridSearchCV...\u001B[37m\u001B[2m\n",
      "Computing metrics...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2mCalculating the accuracy (RMSE, MAE)...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\u001B[37m\u001B[2mBuilding the top-N...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Fitting on the LOOCV...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\u001B[37m\u001B[2m   > Fitting on the full set...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "\u001B[0m\n",
      "\u001B[1mBest params:\u001B[22m \u001B[2m\u001B[37m{'k': 40, 'min_k': 3, 'sim_options': {'name': 'pearson_baseline', 'user_based': True}}\u001B[0m\n",
      "\u001B[1mRMSE:\u001B[22m 2.155113\n",
      "\u001B[1mMAE:\u001B[22m 1.320680\n",
      "\n",
      "\u001B[1mHit rate:\u001B[22m 1.457490%\n",
      "\u001B[1mHit rate per rating value:\u001B[22m\n",
      "Rating\tHit rate\n",
      "7.0\t0.523560%\n",
      "8.0\t0.393701%\n",
      "9.0\t0.862069%\n",
      "10.0\t5.982906%\n",
      "\u001B[1mCumulative hit rate (min_rating=3.0):\u001B[22m 1.757812%\n",
      "\u001B[1mAverage reciprocal hit rank:\u001B[22m 0.005801683696420538\n",
      "\u001B[1mUser coverage (num_users=1235, min_rating=3.0):\u001B[22m 98.947368%\n",
      "\u001B[1mDiversity:\u001B[22m 1.000000\n",
      "\u001B[1mNovelty:\u001B[22m 1440.850299\n",
      "\n",
      "\u001B[0mTesting of the \"KNN With Z-Score\" model successfully completed in 1:25:29.128059.\n",
      "Grid search: 1:18:01.408237\n",
      "Training and testing: 0:00:04.938350\n",
      "Top-N building: 0:07:18.692621\n"
     ]
    }
   ],
   "source": [
    "evaluator.run_model(\n",
    "    name=\"KNN With Z-Score\",\n",
    "    model=surprise.KNNWithZScore,\n",
    "    hyper_params={\n",
    "        \"k\": [20, 40, 60],\n",
    "        \"min_k\": [1, 2, 3, 5],\n",
    "        \"sim_options\": {\n",
    "            \"name\": [\"cosine\", \"msd\", \"pearson\", \"pearson_baseline\"],\n",
    "            \"user_based\": [True, False]\n",
    "        }\n",
    "    },\n",
    "    measure_key=\"rmse\",\n",
    "    override=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.5 - KNN Baseline\n",
    "KNN Baseline is simpler than the previous algorithm. It calculates the distance between the raw values of the features that we want to use for our prediction. The counterpart of this method is the loss of accuracy depending to the scales or ranges of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mTesting \"KNN Baseline\".\u001B[39m\n",
      "Running GridSearchCV...\u001B[37m\u001B[2m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mevaluator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mKNN Baseline\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msurprise\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mKNNBaseline\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhyper_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mk\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m40\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m60\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmin_k\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msim_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mname\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcosine\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmsd\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpearson\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpearson_baseline\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser_based\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m]\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbsl_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmethod\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mals\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn_epochs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmeasure_key\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrmse\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43moverride\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[0;32m     18\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Cours-IA\\5MLRE\\Project\\notebooks\\en\\..\\..\\helpers\\ml.py:165\u001B[0m, in \u001B[0;36mModelEvaluator.run_model\u001B[1;34m(self, name, model, hyper_params, measure_key, override)\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m io\u001B[38;5;241m.\u001B[39mcapture_output():\n\u001B[0;32m    156\u001B[0m     grid_search \u001B[38;5;241m=\u001B[39m surprise\u001B[38;5;241m.\u001B[39mmodel_selection\u001B[38;5;241m.\u001B[39mGridSearchCV(\n\u001B[0;32m    157\u001B[0m         algo_class\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m    158\u001B[0m         param_grid\u001B[38;5;241m=\u001B[39mhyper_params,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    163\u001B[0m         joblib_verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    164\u001B[0m     )\n\u001B[1;32m--> 165\u001B[0m     \u001B[43mgrid_search\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    167\u001B[0m best_model \u001B[38;5;241m=\u001B[39m grid_search\u001B[38;5;241m.\u001B[39mbest_estimator[measure_key]\n\u001B[0;32m    168\u001B[0m best_params \u001B[38;5;241m=\u001B[39m grid_search\u001B[38;5;241m.\u001B[39mbest_params[measure_key]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\5MLRE\\lib\\site-packages\\surprise\\model_selection\\search.py:104\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m     90\u001B[0m cv \u001B[38;5;241m=\u001B[39m get_cv(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcv)\n\u001B[0;32m     92\u001B[0m delayed_list \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     93\u001B[0m     delayed(fit_and_score)(\n\u001B[0;32m     94\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malgo_class(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    102\u001B[0m     )\n\u001B[0;32m    103\u001B[0m )\n\u001B[1;32m--> 104\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    105\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    106\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpre_dispatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpre_dispatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    107\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoblib_verbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    108\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdelayed_list\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    110\u001B[0m (test_measures_dicts, train_measures_dicts, fit_times, test_times) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mout)\n\u001B[0;32m    112\u001B[0m \u001B[38;5;66;03m# test_measures_dicts is a list of dict like this:\u001B[39;00m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;66;03m# [{'mae': 1, 'rmse': 2}, {'mae': 2, 'rmse': 3} ...]\u001B[39;00m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;66;03m# E.g. for 5 splits, the first 5 dicts are for the first param\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;66;03m# (n_parameters_combinations, n_splits). This way we can easily compute\u001B[39;00m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;66;03m# the mean and std dev over all splits or over all param comb.\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\5MLRE\\lib\\site-packages\\joblib\\parallel.py:1088\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1085\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n\u001B[0;32m   1086\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1088\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdispatch_one_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1089\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m   1091\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pre_dispatch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1092\u001B[0m     \u001B[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001B[39;00m\n\u001B[0;32m   1093\u001B[0m     \u001B[38;5;66;03m# No need to wait for async callbacks to trigger to\u001B[39;00m\n\u001B[0;32m   1094\u001B[0m     \u001B[38;5;66;03m# consumption.\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\5MLRE\\lib\\site-packages\\joblib\\parallel.py:901\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m    899\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    900\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 901\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    902\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\5MLRE\\lib\\site-packages\\joblib\\parallel.py:819\u001B[0m, in \u001B[0;36mParallel._dispatch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    817\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    818\u001B[0m     job_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs)\n\u001B[1;32m--> 819\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    820\u001B[0m     \u001B[38;5;66;03m# A job can complete so quickly than its callback is\u001B[39;00m\n\u001B[0;32m    821\u001B[0m     \u001B[38;5;66;03m# called before we get here, causing self._jobs to\u001B[39;00m\n\u001B[0;32m    822\u001B[0m     \u001B[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001B[39;00m\n\u001B[0;32m    823\u001B[0m     \u001B[38;5;66;03m# used (rather than .append) in the following line\u001B[39;00m\n\u001B[0;32m    824\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs\u001B[38;5;241m.\u001B[39minsert(job_idx, job)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\5MLRE\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001B[0m, in \u001B[0;36mSequentialBackend.apply_async\u001B[1;34m(self, func, callback)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_async\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    207\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001B[39;00m\n\u001B[1;32m--> 208\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mImmediateResult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback:\n\u001B[0;32m    210\u001B[0m         callback(result)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\5MLRE\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001B[0m, in \u001B[0;36mImmediateResult.__init__\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    594\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[0;32m    595\u001B[0m     \u001B[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001B[39;00m\n\u001B[0;32m    596\u001B[0m     \u001B[38;5;66;03m# arguments in memory\u001B[39;00m\n\u001B[1;32m--> 597\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\5MLRE\\lib\\site-packages\\joblib\\parallel.py:288\u001B[0m, in \u001B[0;36mBatchedCalls.__call__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    287\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 288\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    289\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\5MLRE\\lib\\site-packages\\joblib\\parallel.py:288\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    287\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 288\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    289\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\5MLRE\\lib\\site-packages\\surprise\\model_selection\\validation.py:173\u001B[0m, in \u001B[0;36mfit_and_score\u001B[1;34m(algo, trainset, testset, measures, return_train_measures)\u001B[0m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Helper method that trains an algorithm and compute accuracy measures on\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;124;03ma testset. Also report train and test times.\u001B[39;00m\n\u001B[0;32m    144\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;124;03m        - The testing time in seconds.\u001B[39;00m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    172\u001B[0m start_fit \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m--> 173\u001B[0m \u001B[43malgo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    174\u001B[0m fit_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_fit\n\u001B[0;32m    175\u001B[0m start_test \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\5MLRE\\lib\\site-packages\\surprise\\prediction_algorithms\\knns.py:284\u001B[0m, in \u001B[0;36mKNNBaseline.fit\u001B[1;34m(self, trainset)\u001B[0m\n\u001B[0;32m    282\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbu, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbi \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_baselines()\n\u001B[0;32m    283\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mby \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mswitch(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbu, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbi)\n\u001B[1;32m--> 284\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msim \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_similarities\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    286\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\5MLRE\\lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py:249\u001B[0m, in \u001B[0;36mAlgoBase.compute_similarities\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    247\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mComputing the \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m similarity matrix...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    248\u001B[0m sim \u001B[38;5;241m=\u001B[39m construction_func[name](\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m--> 249\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mverbose\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m:\n\u001B[0;32m    250\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDone computing similarity matrix.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sim\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "evaluator.run_model(\n",
    "    name=\"KNN Baseline\",\n",
    "    model=surprise.KNNBaseline,\n",
    "    hyper_params={\n",
    "        \"k\": [20, 40, 60],\n",
    "        \"min_k\": [1, 2, 3, 5],\n",
    "        \"sim_options\": {\n",
    "            \"name\": [\"cosine\", \"msd\", \"pearson\", \"pearson_baseline\"],\n",
    "            \"user_based\": [True, False]\n",
    "        },\n",
    "        \"bsl_options\": {\n",
    "            \"method\": [\"als\"],\n",
    "            \"n_epochs\": [5, 10, 15],\n",
    "        }\n",
    "    },\n",
    "    measure_key=\"rmse\",\n",
    "    override=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.6 - Non-negative Matrix Factorization\n",
    "Non-negative Matrix Factorization (NMF) is a technique used to facilitate the interpretation of non-negative matrices*¬π* of data. For this, the algorithm tries to find a way to represent a non-negative matrix in smaller non-negative matrices. It can then better interpret the data structures and its predictions are improved.\n",
    "\n",
    "*1: A non-negative matrix is a matrix where all elements are greater than or equal to zero.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mTesting \"Non-negative Matrix Factorization\".\u001B[39m\n",
      "Running GridSearchCV...\u001B[37m\u001B[2m\n",
      "Computing metrics...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2mCalculating the accuracy (RMSE, MAE)...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2mBuilding the top-N...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Fitting on the LOOCV...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Fitting on the full set...\u001B[37m\u001B[2m\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "\u001B[0m\n",
      "\u001B[1mBest params:\u001B[22m \u001B[2m\u001B[37m{'n_factors': 5, 'n_epochs': 75, 'biased': True}\u001B[0m\n",
      "\u001B[1mRMSE:\u001B[22m 2.541973\n",
      "\u001B[1mMAE:\u001B[22m 1.684747\n",
      "\n",
      "\u001B[1mHit rate:\u001B[22m 1.376518%\n",
      "\u001B[1mHit rate per rating value:\u001B[22m\n",
      "Rating\tHit rate\n",
      "-1.0\t0.961538%\n",
      "6.0\t2.941176%\n",
      "7.0\t1.047120%\n",
      "8.0\t1.181102%\n",
      "9.0\t1.293103%\n",
      "10.0\t2.136752%\n",
      "\u001B[1mCumulative hit rate (min_rating=3.0):\u001B[22m 1.464844%\n",
      "\u001B[1mAverage reciprocal hit rank:\u001B[22m 0.0049411991517254675\n",
      "\u001B[1mUser coverage (num_users=1235, min_rating=3.0):\u001B[22m 100.000000%\n",
      "\u001B[1mDiversity:\u001B[22m 0.667507\n",
      "\u001B[1mNovelty:\u001B[22m 1032.217333\n",
      "\n",
      "\u001B[0mTesting of the \"Non-negative Matrix Factorization\" model successfully completed in 0:07:16.716124.\n",
      "Grid search: 0:05:29.780523\n",
      "Training and testing: 0:00:02.145893\n",
      "Top-N building: 0:01:40.146509\n"
     ]
    }
   ],
   "source": [
    "evaluator.run_model(\n",
    "    name=\"Non-negative Matrix Factorization\",\n",
    "    model=surprise.NMF,\n",
    "    hyper_params={\n",
    "        \"n_factors\": [5, 15, 25],\n",
    "        \"n_epochs\": [25, 50, 75],\n",
    "        \"biased\": [True, False]\n",
    "    },\n",
    "    measure_key=\"rmse\",\n",
    "    override=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.7 - Co-clustering\n",
    "The goal of Co-clustering is to find a way to group similar rows and columns of a matrix, like patterns, to make them more apparent. This method is useful when working on datasets with complex row and column relationships. The algorithm will first find a correlation between the rows and columns of the dataset. It will then use the k-mean method to group the data before interpreting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mTesting \"Co-clustering\".\u001B[39m\n",
      "Running GridSearchCV...\u001B[37m\u001B[2m\n",
      "Computing metrics...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2mCalculating the accuracy (RMSE, MAE)...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2mBuilding the top-N...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Fitting on the LOOCV...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Fitting on the full set...\u001B[37m\u001B[2m\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "\u001B[0m\n",
      "\u001B[1mBest params:\u001B[22m \u001B[2m\u001B[37m{'n_cltr_u': 1, 'n_cltr_i': 1, 'n_epochs': 10}\u001B[0m\n",
      "\u001B[1mRMSE:\u001B[22m 2.312769\n",
      "\u001B[1mMAE:\u001B[22m 1.504430\n",
      "\n",
      "\u001B[1mHit rate:\u001B[22m 2.105263%\n",
      "\u001B[1mHit rate per rating value:\u001B[22m\n",
      "Rating\tHit rate\n",
      "6.0\t1.470588%\n",
      "7.0\t0.523560%\n",
      "8.0\t2.362205%\n",
      "9.0\t2.155172%\n",
      "10.0\t5.555556%\n",
      "\u001B[1mCumulative hit rate (min_rating=3.0):\u001B[22m 2.539062%\n",
      "\u001B[1mAverage reciprocal hit rank:\u001B[22m 0.006970953023584602\n",
      "\u001B[1mUser coverage (num_users=1235, min_rating=3.0):\u001B[22m 93.441296%\n",
      "\u001B[1mDiversity:\u001B[22m 0.977778\n",
      "\u001B[1mNovelty:\u001B[22m 2389.190310\n",
      "\n",
      "\u001B[0mTesting of the \"Co-clustering\" model successfully completed in 0:13:19.172950.\n",
      "Grid search: 0:12:00.135119\n",
      "Training and testing: 0:00:00.989192\n",
      "Top-N building: 0:01:13.396222\n"
     ]
    }
   ],
   "source": [
    "evaluator.run_model(\n",
    "    name=\"Co-clustering\",\n",
    "    model=surprise.CoClustering,\n",
    "    hyper_params={\n",
    "        \"n_cltr_u\": [1, 3, 5],\n",
    "        \"n_cltr_i\": [1, 3, 5],\n",
    "        \"n_epochs\": [10, 20, 30],\n",
    "    },\n",
    "    measure_key=\"rmse\",\n",
    "    override=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.8 - Comparing performance\n",
    "Before we compare all the previous results, let's define a few important terms that we need to understand to properly compare the performance of our models.\n",
    "\n",
    "<u>Machine Learning metrics:</u>\n",
    "- Root Mean Squared Error (RMSE): measure of the average deviation of the predicted values of the model. The lower, the better.\n",
    "- Mean Absolute Error (MAE): refers to the magnitude of difference between the prediction of an observation and the true value of that observation.\n",
    "\n",
    "<u>Recommendation systems metrics:</u>\n",
    "- Hit rate: the proportion of recommended items that are relevant to the user, expressed in percent.\n",
    "- Hit rate per rating value: is the hit rate but calculated independently for each of the possible ratings (from one to ten in our case).\n",
    "- Cumulative hit rate: is also the hit rate calculated for all rating values up to a certain threshold.\n",
    "- Average reciprocal hit rank (ARHR): is the average of the reciprocal ranks of the relevant items in the recommended list.\n",
    "- User coverage: is the proportion of users for whom the system is able to make recommendations.\n",
    "- Diversity: is the variety or dissimilarity of items recommended to users.\n",
    "- Novelty: is the degree to which recommended items are new or unexpected to the user.\n",
    "\n",
    "We can now compare our models. We are going to observe which is the best model for each metrics, and then conclude on the overall best model.\n",
    "\n",
    "The model with the most interesting metric values is the KNN Baseline due to its good performance on RMSE, MAE and hit rate. It has a reasonable training time of seven seconds. Even though the grid search takes three hours, we already have the best parameters for this model, so we don't need to run the grid search again. In another hand, KNN With Means show us an interesting performance for ARHR and a training of three seconds. KNN Basic is automatically excluded because 35% of its predictions were impossible (one of the parameters was unknown to it).\n",
    "\n",
    "Overall, based on the metrics and results we have at this moment of the training, KNN Baseline appears to be the most efficient model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E - Collaborative filtering: filtered training\n",
    "We will now run our previous experiment again, but this time using a filtered dataset.\n",
    "\n",
    "### E.1 - Filtering the dataset\n",
    "We only keep users with at least two hundred ratings and items with at least two hundred ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = pandas.read_csv((DATA_FOLDER / \"rating.csv\"), dtype={\"user_id\": str, \"anime_id\": str})\n",
    "\n",
    "# Calculate the data filters\n",
    "ratings_per_users = data_filtered.value_counts(subset=\"user_id\").reset_index(name=\"count\")\n",
    "ratings_per_anime = data_filtered.value_counts(subset=\"anime_id\").reset_index(name=\"count\")\n",
    "\n",
    "# Apply the filters\n",
    "min_n_ratings = 200\n",
    "data_filtered = data_filtered[data_filtered[\"user_id\"].isin(ratings_per_users[ratings_per_users[\"count\"] > min_n_ratings][\"user_id\"])]\n",
    "data_filtered = data_filtered[data_filtered[\"anime_id\"].isin(ratings_per_anime[ratings_per_anime[\"count\"] > min_n_ratings][\"anime_id\"])]\n",
    "\n",
    "# Save the file\n",
    "data_filtered.to_csv((DATA_FOLDER / \"rating_filtered_min_rating_200.csv\"), index=False, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the filtered dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = pandas.read_csv((DATA_FOLDER / \"rating_filtered_min_rating_200.csv\"), dtype={\"user_id\": str, \"anime_id\": str})\n",
    "data_filtered = data_filtered.head(n=125_000)\n",
    "\n",
    "data_reader = surprise.Reader(rating_scale=(-1, 10))\n",
    "data_filtered = surprise.Dataset.load_from_df(df=data_filtered[[\"user_id\", \"anime_id\", \"rating\"]], reader=data_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we initialize a new evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing sets. This can take a while...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Building train/test sets...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Building LeaveOneOut sets...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Building full sets...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Preparing the similarities model...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "evaluator_filtered = helpers.ml.ModelEvaluator(dataset=data_filtered, rankings=rankings, models_folder=MODELS_FOLDER, seed=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now redefine our previous model and train them a second time with this new dataset.\n",
    "\n",
    "### E.2 - Slope One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mTesting \"Slope One - Filtered\".\u001B[39m\n",
      "Computing metrics...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2mCalculating the accuracy (RMSE, MAE)...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2mBuilding the top-N...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Fitting on the LOOCV...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Fitting on the full set...\u001B[37m\u001B[2m\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "\u001B[0m\n",
      "\u001B[1mBest params:\u001B[22m \u001B[2m\u001B[37m{}\u001B[0m\n",
      "\u001B[1mRMSE:\u001B[22m 1.945212\n",
      "\u001B[1mMAE:\u001B[22m 1.246553\n",
      "\n",
      "\u001B[1mHit rate:\u001B[22m 0.000000%\n",
      "\u001B[1mHit rate per rating value:\u001B[22m\n",
      "Rating\tHit rate\n",
      "\u001B[1mCumulative hit rate (min_rating=3.0):\u001B[22m 0.000000%\n",
      "\u001B[1mAverage reciprocal hit rank:\u001B[22m 0\n",
      "\u001B[1mUser coverage (num_users=379, min_rating=3.0):\u001B[22m 98.153034%\n",
      "\u001B[1mDiversity:\u001B[22m 0.888889\n",
      "\u001B[1mNovelty:\u001B[22m 2299.656091\n",
      "\n",
      "\u001B[0mTesting of the \"Slope One - Filtered\" model successfully completed in 0:05:39.533346.\n",
      "Grid search: N/A\n",
      "Training and testing: 0:00:05.747381\n",
      "Top-N building: 0:05:29.728253\n"
     ]
    }
   ],
   "source": [
    "evaluator_filtered.run_model(name=\"Slope One - Filtered\", model=surprise.SlopeOne, hyper_params=None, measure_key=\"rmse\", override=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.3 - KNN Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mTesting \"KNN Basic - Filtered\".\u001B[39m\n",
      "Running GridSearchCV...\u001B[37m\u001B[2m\n",
      "Computing metrics...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2mCalculating the accuracy (RMSE, MAE)...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\u001B[37m\u001B[2mBuilding the top-N...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Fitting on the LOOCV...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\u001B[37m\u001B[2m   > Fitting on the full set...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\u001B[33mWarning: 49862/1211354 (4.116220%) predictions were impossible! \u001B[39m\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "\u001B[33mWarning: 49854/1210975 (4.116848%) predictions were impossible! \u001B[39m\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "\u001B[0m\n",
      "\u001B[1mBest params:\u001B[22m \u001B[2m\u001B[37m{'k': 40, 'min_k': 1, 'sim_options': {'name': 'pearson_baseline', 'user_based': False}}\u001B[0m\n",
      "\u001B[1mRMSE:\u001B[22m 1.928885\n",
      "\u001B[1mMAE:\u001B[22m 1.176739\n",
      "\n",
      "\u001B[1mHit rate:\u001B[22m 0.791557%\n",
      "\u001B[1mHit rate per rating value:\u001B[22m\n",
      "Rating\tHit rate\n",
      "10.0\t6.666667%\n",
      "\u001B[1mCumulative hit rate (min_rating=3.0):\u001B[22m 1.006711%\n",
      "\u001B[1mAverage reciprocal hit rank:\u001B[22m 0.001978891820580475\n",
      "\u001B[1mUser coverage (num_users=379, min_rating=3.0):\u001B[22m 100.000000%\n",
      "\u001B[1mDiversity:\u001B[22m 0.644532\n",
      "\u001B[1mNovelty:\u001B[22m 2853.227441\n",
      "\n",
      "\u001B[0mTesting of the \"KNN Basic - Filtered\" model successfully completed in 1:16:08.922350.\n",
      "Grid search: 1:08:19.902266\n",
      "Training and testing: 0:00:09.668878\n",
      "Top-N building: 0:07:35.206491\n"
     ]
    }
   ],
   "source": [
    "evaluator_filtered.run_model(\n",
    "    name=\"KNN Basic - Filtered\",\n",
    "    model=surprise.KNNBasic,\n",
    "    hyper_params={\n",
    "        \"k\": [20, 40, 60],\n",
    "        \"min_k\": [1, 2, 3, 5],\n",
    "        \"sim_options\": {\n",
    "            \"name\": [\"cosine\", \"msd\", \"pearson\", \"pearson_baseline\"],\n",
    "            \"user_based\": [True, False]\n",
    "        }\n",
    "    },\n",
    "    measure_key=\"rmse\",\n",
    "    override=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.4 - KNN With Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mTesting \"KNN With Means - Filtered\".\u001B[39m\n",
      "Running GridSearchCV...\u001B[37m\u001B[2m\n",
      "Computing metrics...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2mCalculating the accuracy (RMSE, MAE)...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\u001B[37m\u001B[2mBuilding the top-N...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Fitting on the LOOCV...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\u001B[37m\u001B[2m   > Fitting on the full set...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "\u001B[0m\n",
      "\u001B[1mBest params:\u001B[22m \u001B[2m\u001B[37m{'k': 40, 'min_k': 3, 'sim_options': {'name': 'pearson_baseline', 'user_based': True}}\u001B[0m\n",
      "\u001B[1mRMSE:\u001B[22m 1.870636\n",
      "\u001B[1mMAE:\u001B[22m 1.156339\n",
      "\n",
      "\u001B[1mHit rate:\u001B[22m 0.263852%\n",
      "\u001B[1mHit rate per rating value:\u001B[22m\n",
      "Rating\tHit rate\n",
      "10.0\t2.222222%\n",
      "\u001B[1mCumulative hit rate (min_rating=3.0):\u001B[22m 0.335570%\n",
      "\u001B[1mAverage reciprocal hit rank:\u001B[22m 0.00032981530343007914\n",
      "\u001B[1mUser coverage (num_users=379, min_rating=3.0):\u001B[22m 96.042216%\n",
      "\u001B[1mDiversity:\u001B[22m 0.568384\n",
      "\u001B[1mNovelty:\u001B[22m 1708.212530\n",
      "\n",
      "\u001B[0mTesting of the \"KNN With Means - Filtered\" model successfully completed in 1:12:06.146278.\n",
      "Grid search: 1:10:30.125031\n",
      "Training and testing: 0:00:02.997573\n",
      "Top-N building: 0:01:28.886466\n"
     ]
    }
   ],
   "source": [
    "evaluator_filtered.run_model(\n",
    "    name=\"KNN With Means - Filtered\",\n",
    "    model=surprise.KNNWithMeans,\n",
    "    hyper_params={\n",
    "        \"k\": [20, 40, 60],\n",
    "        \"min_k\": [1, 2, 3, 5],\n",
    "        \"sim_options\": {\n",
    "            \"name\": [\"cosine\", \"msd\", \"pearson\", \"pearson_baseline\"],\n",
    "            \"user_based\": [True, False]\n",
    "        }\n",
    "    },\n",
    "    measure_key=\"rmse\",\n",
    "    override=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.5 - KNN With Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mTesting \"KNN With Z-Score - Filtered\".\u001B[39m\n",
      "Running GridSearchCV...\u001B[37m\u001B[2m\n",
      "Computing metrics...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2mCalculating the accuracy (RMSE, MAE)...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\u001B[37m\u001B[2mBuilding the top-N...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Fitting on the LOOCV...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\u001B[37m\u001B[2m   > Fitting on the full set...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "\u001B[0m\n",
      "\u001B[1mBest params:\u001B[22m \u001B[2m\u001B[37m{'k': 40, 'min_k': 3, 'sim_options': {'name': 'pearson_baseline', 'user_based': True}}\u001B[0m\n",
      "\u001B[1mRMSE:\u001B[22m 1.868933\n",
      "\u001B[1mMAE:\u001B[22m 1.136648\n",
      "\n",
      "\u001B[1mHit rate:\u001B[22m 1.055409%\n",
      "\u001B[1mHit rate per rating value:\u001B[22m\n",
      "Rating\tHit rate\n",
      "9.0\t1.612903%\n",
      "10.0\t6.666667%\n",
      "\u001B[1mCumulative hit rate (min_rating=3.0):\u001B[22m 1.342282%\n",
      "\u001B[1mAverage reciprocal hit rank:\u001B[22m 0.0024699501612430374\n",
      "\u001B[1mUser coverage (num_users=379, min_rating=3.0):\u001B[22m 97.625330%\n",
      "\u001B[1mDiversity:\u001B[22m 0.391335\n",
      "\u001B[1mNovelty:\u001B[22m 1583.602421\n",
      "\n",
      "\u001B[0mTesting of the \"KNN With Z-Score - Filtered\" model successfully completed in 1:14:58.682461.\n",
      "Grid search: 1:13:17.586705\n",
      "Training and testing: 0:00:03.167538\n",
      "Top-N building: 0:01:33.786858\n"
     ]
    }
   ],
   "source": [
    "evaluator_filtered.run_model(\n",
    "    name=\"KNN With Z-Score - Filtered\",\n",
    "    model=surprise.KNNWithZScore,\n",
    "    hyper_params={\n",
    "        \"k\": [20, 40, 60],\n",
    "        \"min_k\": [1, 2, 3, 5],\n",
    "        \"sim_options\": {\n",
    "            \"name\": [\"cosine\", \"msd\", \"pearson\", \"pearson_baseline\"],\n",
    "            \"user_based\": [True, False]\n",
    "        }\n",
    "    },\n",
    "    measure_key=\"rmse\",\n",
    "    override=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.6 - KNN Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mTesting \"KNN Baseline - Filtered\".\u001B[39m\n",
      "Running GridSearchCV...\u001B[37m\u001B[2m\n",
      "Computing metrics...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2mCalculating the accuracy (RMSE, MAE)...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\u001B[37m\u001B[2mBuilding the top-N...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Fitting on the LOOCV...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\u001B[37m\u001B[2m   > Fitting on the full set...\u001B[37m\u001B[2m\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "\u001B[0m\n",
      "\u001B[1mBest params:\u001B[22m \u001B[2m\u001B[37m{'k': 20, 'min_k': 5, 'sim_options': {'name': 'pearson_baseline', 'user_based': False}, 'bsl_options': {'method': 'als', 'n_epochs': 15}}\u001B[0m\n",
      "\u001B[1mRMSE:\u001B[22m 1.830114\n",
      "\u001B[1mMAE:\u001B[22m 1.100667\n",
      "\n",
      "\u001B[1mHit rate:\u001B[22m 2.374670%\n",
      "\u001B[1mHit rate per rating value:\u001B[22m\n",
      "Rating\tHit rate\n",
      "9.0\t3.225806%\n",
      "10.0\t15.555556%\n",
      "\u001B[1mCumulative hit rate (min_rating=3.0):\u001B[22m 3.020134%\n",
      "\u001B[1mAverage reciprocal hit rank:\u001B[22m 0.010920551158018177\n",
      "\u001B[1mUser coverage (num_users=379, min_rating=3.0):\u001B[22m 87.335092%\n",
      "\u001B[1mDiversity:\u001B[22m 0.128882\n",
      "\u001B[1mNovelty:\u001B[22m 690.306397\n",
      "\n",
      "\u001B[0mTesting of the \"KNN Baseline - Filtered\" model successfully completed in 3:53:54.854782.\n",
      "Grid search: 3:46:48.569971\n",
      "Training and testing: 0:00:09.270283\n",
      "Top-N building: 0:06:52.871772\n"
     ]
    }
   ],
   "source": [
    "evaluator_filtered.run_model(\n",
    "    name=\"KNN Baseline - Filtered\",\n",
    "    model=surprise.KNNBaseline,\n",
    "    hyper_params={\n",
    "        \"k\": [20, 40, 60],\n",
    "        \"min_k\": [1, 2, 3, 5],\n",
    "        \"sim_options\": {\n",
    "            \"name\": [\"cosine\", \"msd\", \"pearson\", \"pearson_baseline\"],\n",
    "            \"user_based\": [True, False]\n",
    "        },\n",
    "        \"bsl_options\": {\n",
    "            \"method\": [\"als\"],\n",
    "            \"n_epochs\": [5, 10, 15],\n",
    "        }\n",
    "    },\n",
    "    measure_key=\"rmse\",\n",
    "    override=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.7 - Non-negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mTesting \"Non-negative Matrix Factorization - Filtered\".\u001B[39m\n",
      "Running GridSearchCV...\u001B[37m\u001B[2m\n",
      "Computing metrics...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2mCalculating the accuracy (RMSE, MAE)...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2mBuilding the top-N...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Fitting on the LOOCV...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Fitting on the full set...\u001B[37m\u001B[2m\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "\u001B[0m\n",
      "\u001B[1mBest params:\u001B[22m \u001B[2m\u001B[37m{'n_factors': 5, 'n_epochs': 25, 'biased': True}\u001B[0m\n",
      "\u001B[1mRMSE:\u001B[22m 2.150025\n",
      "\u001B[1mMAE:\u001B[22m 1.405496\n",
      "\n",
      "\u001B[1mHit rate:\u001B[22m 0.263852%\n",
      "\u001B[1mHit rate per rating value:\u001B[22m\n",
      "Rating\tHit rate\n",
      "8.0\t1.111111%\n",
      "\u001B[1mCumulative hit rate (min_rating=3.0):\u001B[22m 0.335570%\n",
      "\u001B[1mAverage reciprocal hit rank:\u001B[22m 0.0002638522427440633\n",
      "\u001B[1mUser coverage (num_users=379, min_rating=3.0):\u001B[22m 96.042216%\n",
      "\u001B[1mDiversity:\u001B[22m 0.059527\n",
      "\u001B[1mNovelty:\u001B[22m 759.867948\n",
      "\n",
      "\u001B[0mTesting of the \"Non-negative Matrix Factorization - Filtered\" model successfully completed in 0:05:23.866994.\n",
      "Grid search: 0:05:00.347844\n",
      "Training and testing: 0:00:00.808898\n",
      "Top-N building: 0:00:18.562031\n"
     ]
    }
   ],
   "source": [
    "evaluator_filtered.run_model(\n",
    "    name=\"Non-negative Matrix Factorization - Filtered\",\n",
    "    model=surprise.NMF,\n",
    "    hyper_params={\n",
    "        \"n_factors\": [5, 15, 25],\n",
    "        \"n_epochs\": [25, 50, 75],\n",
    "        \"biased\": [True, False]\n",
    "    },\n",
    "    measure_key=\"rmse\",\n",
    "    override=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.8 - Co-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mTesting \"Co-clustering - Filtered\".\u001B[39m\n",
      "Running GridSearchCV...\u001B[37m\u001B[2m\n",
      "Computing metrics...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2mCalculating the accuracy (RMSE, MAE)...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2mBuilding the top-N...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Fitting on the LOOCV...\u001B[37m\u001B[2m\n",
      "\u001B[37m\u001B[2m   > Fitting on the full set...\u001B[37m\u001B[2m\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "Built top-N for each user (n=10, min_rating=3.0)\n",
      "\u001B[0m\n",
      "\u001B[1mBest params:\u001B[22m \u001B[2m\u001B[37m{'n_cltr_u': 1, 'n_cltr_i': 1, 'n_epochs': 10}\u001B[0m\n",
      "\u001B[1mRMSE:\u001B[22m 2.034271\n",
      "\u001B[1mMAE:\u001B[22m 1.326865\n",
      "\n",
      "\u001B[1mHit rate:\u001B[22m 0.263852%\n",
      "\u001B[1mHit rate per rating value:\u001B[22m\n",
      "Rating\tHit rate\n",
      "9.0\t1.612903%\n",
      "\u001B[1mCumulative hit rate (min_rating=3.0):\u001B[22m 0.335570%\n",
      "\u001B[1mAverage reciprocal hit rank:\u001B[22m 0.00032981530343007914\n",
      "\u001B[1mUser coverage (num_users=379, min_rating=3.0):\u001B[22m 100.000000%\n",
      "\u001B[1mDiversity:\u001B[22m 0.933333\n",
      "\u001B[1mNovelty:\u001B[22m 1976.632591\n",
      "\n",
      "\u001B[0mTesting of the \"Co-clustering - Filtered\" model successfully completed in 0:11:14.688600.\n",
      "Grid search: 0:10:55.369773\n",
      "Training and testing: 0:00:00.851703\n",
      "Top-N building: 0:00:14.339976\n"
     ]
    }
   ],
   "source": [
    "evaluator_filtered.run_model(\n",
    "    name=\"Co-clustering - Filtered\",\n",
    "    model=surprise.CoClustering,\n",
    "    hyper_params={\n",
    "        \"n_cltr_u\": [1, 3, 5],\n",
    "        \"n_cltr_i\": [1, 3, 5],\n",
    "        \"n_epochs\": [10, 20, 30],\n",
    "    },\n",
    "    measure_key=\"rmse\",\n",
    "    override=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### C.9 - Comparing performance\n",
    "The models show similar results to the previous run on unfiltered data. But, if we look closely to the metrics, we easily see that most of the scores are worse than ever. We already have a really low hit rate and the filtered dataset doesn't help in this situation.\n",
    "\n",
    "However, the model KNN Baseline is still the best."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## E - ???\n",
    "We didn't expected to get result as low as we got in this notebook. Most of our previous tests with the MovieLens dataset gave much better results, even the simplest model. We tried multiple things to improve the scores, none of them actually worked. But, we will talk about it anyway.\n",
    "\n",
    "The first runs were using the original dataset. A major portion of this set is full of `-1`, a rating that indicate that the user gave no rating. It's still the method used in this notebook and you can notice that it doesn't work very well.\n",
    "\n",
    "Well, maybe the negative ratings are causing trouble during the models training? They could see these ratings as very bad items that musn't be recommended and then recommend items with better average rating but that do not really fit what the user likes. For the next runs, we decided to filter out these ratings. We train the models once more, and, once again, they were pretty bad at giving relevant predictions. It was even worse than before.\n",
    "\n",
    "Keeping the negatives ratings gives a better result, but the trained models do not give predictions that fit the user needs. Removing them reduce the amount of data and some user are even entirely excluded from the prediction system. Why don't we try both at once ? For the last runs, we decide to keep every rows, but we replaced the `-1` by the median of the range `(1, 10)`. We can judge that if a user didn't gave a note, it wasn't an anime enough bad or good to be rated. Surprisingly, it was worse than ever. Worse than the two previous experiments. Most models were getting a hit rate lower than one percent.\n",
    "\n",
    "We're not really sure why we got those scores and, unfortunately, we ran out of time and idea. For now, our best model is the KNN Baseline with three percent of hit rate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D - Getting the Top-N\n",
    "The final step is to display the top-N of a user. We start by loading the previously saved top of our best model. To compare a bit more our models, we will take the two best of the unfiltered and the filtered dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_knn_baseline = helpers.ml.Model.load_top_n(filepath=(MODELS_FOLDER / \"KNN_Baseline__topN-full.pkl\"))\n",
    "top_n_filtered_knn_baseline = helpers.ml.Model.load_top_n(filepath=(MODELS_FOLDER / \"KNN_Baseline_-_Filtered__topN-full.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pick a random user from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "933"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_user_id = int(random.choice(list(set([r[0] for r in data.raw_ratings]))))\n",
    "random_user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that build a human-readable table from the top-N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_of(user_id: int, top_n: dict[int, list], items_df: pandas.DataFrame, auto_print: bool = False) -> pandas.DataFrame:\n",
    "    \"\"\" Returns the formatted top-N recommendation for a specific user. \"\"\"\n",
    "    user_top = []\n",
    "\n",
    "    for top_item_id, estimated_rating, _ in top_n[user_id]:\n",
    "        item = items_df[items_df[\"anime_id\"] == top_item_id].iloc[0]\n",
    "        user_top.append({\n",
    "            \"Name\": item[\"name\"],\n",
    "            \"Genre\": item[\"genre\"],\n",
    "            \"Num. ratings\": item[\"num_ratings\"],\n",
    "            \"Mean ratings\": item[\"rating\"],\n",
    "            \"User estimated rating\": estimated_rating\n",
    "        })\n",
    "\n",
    "    return pandas.DataFrame(data=user_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mTop-N: unfiltered dataset\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                      Name  \\\n0                   Hunter x Hunter (2011)   \n1                                  Gintama   \n2                      Howl no Ugoku Shiro   \n3                                  Monster   \n4                             Cowboy Bebop   \n5                  Boku dake ga Inai Machi   \n6                           Shinsekai yori   \n7  Gyakkyou Burai Kaiji: Ultimate Survivor   \n8                                 Gintama¬∞   \n9              Kuroko no Basket 2nd Season   \n\n                                               Genre  Num. ratings  \\\n0            Action, Adventure, Shounen, Super Power          7477   \n1  Action, Comedy, Historical, Parody, Samurai, S...          4264   \n2                 Adventure, Drama, Fantasy, Romance         14560   \n3  Drama, Horror, Mystery, Police, Psychological,...          4079   \n4    Action, Adventure, Comedy, Drama, Sci-Fi, Space         13449   \n5       Mystery, Psychological, Seinen, Supernatural          7991   \n6       Drama, Horror, Mystery, Sci-Fi, Supernatural          5485   \n7              Game, Psychological, Seinen, Thriller          2653   \n8  Action, Comedy, Historical, Parody, Samurai, S...          1188   \n9                    Comedy, School, Shounen, Sports          6819   \n\n   Mean ratings  User estimated rating  \n0          9.13              10.000000  \n1          9.04               9.786675  \n2          8.74               9.775792  \n3          8.72               9.774643  \n4          8.82               9.762582  \n5          8.65               9.741796  \n6          8.53               9.733932  \n7          8.33               9.729091  \n8          9.25               9.630482  \n9          8.58               9.587015  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Genre</th>\n      <th>Num. ratings</th>\n      <th>Mean ratings</th>\n      <th>User estimated rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hunter x Hunter (2011)</td>\n      <td>Action, Adventure, Shounen, Super Power</td>\n      <td>7477</td>\n      <td>9.13</td>\n      <td>10.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Gintama</td>\n      <td>Action, Comedy, Historical, Parody, Samurai, S...</td>\n      <td>4264</td>\n      <td>9.04</td>\n      <td>9.786675</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Howl no Ugoku Shiro</td>\n      <td>Adventure, Drama, Fantasy, Romance</td>\n      <td>14560</td>\n      <td>8.74</td>\n      <td>9.775792</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Monster</td>\n      <td>Drama, Horror, Mystery, Police, Psychological,...</td>\n      <td>4079</td>\n      <td>8.72</td>\n      <td>9.774643</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Cowboy Bebop</td>\n      <td>Action, Adventure, Comedy, Drama, Sci-Fi, Space</td>\n      <td>13449</td>\n      <td>8.82</td>\n      <td>9.762582</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Boku dake ga Inai Machi</td>\n      <td>Mystery, Psychological, Seinen, Supernatural</td>\n      <td>7991</td>\n      <td>8.65</td>\n      <td>9.741796</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Shinsekai yori</td>\n      <td>Drama, Horror, Mystery, Sci-Fi, Supernatural</td>\n      <td>5485</td>\n      <td>8.53</td>\n      <td>9.733932</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Gyakkyou Burai Kaiji: Ultimate Survivor</td>\n      <td>Game, Psychological, Seinen, Thriller</td>\n      <td>2653</td>\n      <td>8.33</td>\n      <td>9.729091</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Gintama¬∞</td>\n      <td>Action, Comedy, Historical, Parody, Samurai, S...</td>\n      <td>1188</td>\n      <td>9.25</td>\n      <td>9.630482</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Kuroko no Basket 2nd Season</td>\n      <td>Comedy, School, Shounen, Sports</td>\n      <td>6819</td>\n      <td>8.58</td>\n      <td>9.587015</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"{Style.BRIGHT}Top-N: unfiltered dataset{Style.RESET_ALL}\")\n",
    "get_top_n_of(user_id=random_user_id, top_n=top_n_knn_baseline, items_df=data_anime, auto_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mTop-N: filtered dataset\u001B[0m\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "933",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mStyle\u001B[38;5;241m.\u001B[39mBRIGHT\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mTop-N: filtered dataset\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mStyle\u001B[38;5;241m.\u001B[39mRESET_ALL\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mget_top_n_of\u001B[49m\u001B[43m(\u001B[49m\u001B[43muser_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_user_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_n\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_n_filtered_knn_baseline\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mitems_df\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_anime\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mauto_print\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[12], line 5\u001B[0m, in \u001B[0;36mget_top_n_of\u001B[1;34m(user_id, top_n, items_df, auto_print)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\" Returns the formatted top-N recommendation for a specific user. \"\"\"\u001B[39;00m\n\u001B[0;32m      3\u001B[0m user_top \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m top_item_id, estimated_rating, _ \u001B[38;5;129;01min\u001B[39;00m \u001B[43mtop_n\u001B[49m\u001B[43m[\u001B[49m\u001B[43muser_id\u001B[49m\u001B[43m]\u001B[49m:\n\u001B[0;32m      6\u001B[0m     item \u001B[38;5;241m=\u001B[39m items_df[items_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124manime_id\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m top_item_id]\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m      7\u001B[0m     user_top\u001B[38;5;241m.\u001B[39mappend({\n\u001B[0;32m      8\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mName\u001B[39m\u001B[38;5;124m\"\u001B[39m: item[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m      9\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenre\u001B[39m\u001B[38;5;124m\"\u001B[39m: item[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenre\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUser estimated rating\u001B[39m\u001B[38;5;124m\"\u001B[39m: estimated_rating\n\u001B[0;32m     13\u001B[0m     })\n",
      "\u001B[1;31mKeyError\u001B[0m: 933"
     ]
    }
   ],
   "source": [
    "print(f\"{Style.BRIGHT}Top-N: filtered dataset{Style.RESET_ALL}\")\n",
    "get_top_n_of(user_id=random_user_id, top_n=top_n_filtered_knn_baseline, items_df=data_anime, auto_print=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the top-10 of the randomly selected user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E - Conclusion of the collaborative filtering\n",
    "**TODO: Add text**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
